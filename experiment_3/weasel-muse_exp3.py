import os 
import numpy as np
import pandas as pd
import glob
from tslearn.preprocessing import TimeSeriesResampler
from scipy import signal
from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit
from pyts.multivariate.transformation import WEASELMUSE
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from scipy.signal import medfilt
from tqdm import tqdm
from math import sqrt 
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
import pickle 
from tslearn.utils import to_pyts_dataset 

main_path = 'SisFall_dataset/'
samp_rate = 200
n_timestamps = 36000
sensor = ["XAD", "YAD", "ZAD", "XR", "YR", "ZR", "XM", "YM", "ZM"]
#find t at max magnitude : ax^2 + ay^2 + az^2 
chosen = ["XAD", "ZAD", "XR" ,"YR", "ZR"]
dataf =  [ ]
#train model
def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))

    return df

def get_data(path):
    """
    read and processing data
    return data (ndarray) shape = (n_features, n_timestamps)
    """
    df = pd.read_csv(path, delimiter=',', header=None)
    # df = reduce_mem_usage(df)
    dataf = pd.read_csv(path, delimiter=',', header=None)
    dataf.columns = sensor

    df.columns = sensor
    df['ZM'] = df['ZM'].replace({';': ''}, regex=True)
    data = df[chosen].values.T # shape = (n_features, n_timestamps)
    dataf = df[sensor].values.T
    si = (data.shape[-1] // 200) * samp_rate
    data = signal.resample(x=data, num=si, axis=1)
    dataf = signal.resample(x=dataf, num=si, axis=1)
    data = np.pad(data, ((0, 0), (0, n_timestamps-data.shape[-1])), 'constant') # pad zero
    data = medfilt(data, kernel_size=(1,3))

    dataf = np.pad(dataf, ((0, 0), (0, n_timestamps-dataf.shape[-1])), 'constant')
    dataf = medfilt(dataf, kernel_size=(1,3))
    # data = medfilt(data, kernel_size=1)
    return data, dataf # shape = (n_features, n_timestamps)


def get_meta(path):
    """
    get list of metadata from each file
    """
    f = path.split('/')[-1].replace('.txt', '') # D01_SA01_R01
    activity, subject, record = f.split('_') # [D01, SA01, R01]
    label = activity[0] # A or D
    return [label, activity, subject, record]

def load_dataset():
    path_list = glob.glob(main_path+'*/*.txt')
    X, y, meta = [], [], []
    fullX = [] 
 
    for path in tqdm(path_list):
        data_ , dataf_= get_data(path)
        meta_ = get_meta(path)
        
        X.append(data_)
        y.append(meta_[0])
        fullX.append(dataf_)
        meta.append(meta_)
    return np.array(X), np.array(y), np.array(meta), np.array(fullX)

current = 0
current_sample = 0 
# def find_peak() : 


if __name__ == "__main__":

    # f = open("Results Experiment 1.txt", 'w')
    X, y, meta, fullX = load_dataset() 

    subjects_id = np.unique(meta[:,2])
    SA_id = subjects_id[:23]
    SE_id = subjects_id[23:]

    # X = X[ : , : , : 6000]
    threshold = 0.39145434515803695
    
    print(X.shape, y.shape, meta.shape, fullX.shape)
    print('\n', subjects_id, '\n', SA_id, '\n', SE_id)

    window_lengths =  [ 0.1 , 0.3 , 0.5 , 0.7 , 0.9] 
    # listlatency = [92.5, 497.5, 437.5, 30.0, 65.0, 410.0, 645.0, 2060.0, 457.5, 527.5, 572.5, 1182.5, 735.0, 765.0, 507.5, 520.0, 542.5, 355.0, 197.5, 332.5, 802.5, 200.0, 582.5, 325.0, 327.5, 212.5, 515.0, 382.5, 570.0, 5545.0, 10.0, 520.0, 317.5, 387.5, 342.5, 490.0, 395.0, 362.5, 5510.0, 490.0, 527.5, 407.5, 510.0, 10.0, 290.0, 287.5, 195.0, 537.5, 1062.5, 47.5, 1130.0, 2.5, 17.5, 5.0, 10.0, 450.0, 1000.0, 505.0, 380.0, 20.0, 7.5, 410.0, 1835.0, 462.5, 507.5, 7.5, 372.5, 742.5, 615.0, 547.5, 2.5, 1305.0, 359.9999999999991, 447.5, 527.5, 182.5, 470.0, 355.0, 340.0, 5.0, 455.0000000000009, 410.0, 860.0, 340.0000000000009, 17.5, 332.5, 7.5, 455.0, 2405.0, 440.0, 142.5, 317.5, 455.0, 3300.0, 502.5, 0.0, 532.5, 2517.5, 1322.5, 335.0, 510.0, 462.5, 12.5, 457.5, 2015.0, 365.0, 1157.5, 562.5, 500.0, 20.0, 505.0, 397.5, 615.0, 375.0, 230.0, 4852.5, 5.0, 332.5, 487.5, 262.4999999999991,492.5, 275.0, 15.0, 510.0, 720.0, 492.5, 560.0, 165.0, 407.5, 75.0, 52.5, 1277.5, 407.5,452.5, 540.0, 560.0, 495.0, 412.5, 27.5, 22.5, 1037.500000000001,265.0,485.0, 50.0, 22.5, 202.5, 532.5,390.0, 397.5, 527.5, 535.0, 275.0, 107.5, 282.5, 850.0, 502.5, 307.5, 500.0, 105.0, 505.0, 4419.999999999999, 490.0, 530.0000000000009, 477.4999999999991,475.0, 590.0, 875.0000000000009, 510.0, 600.0, 52.5, 470.0, 702.5000000000009, 502.5, 270.0, 550.0, 472.5, 490.0,60.0, 492.5, 17.5, 12.5, 2445.0, 632.5, 295.0, 0.0,97.5, 257.5, 362.5, 532.5000000000009, 522.5, 505.0, 502.5, 525.0, 542.5, 1337.5,435.0, 1252.5, 512.5,515.0, 542.5000000000009, 510.0, 492.5, 505.0, 527.5, 577.5, 395.0,237.5, 160.0, 487.5, 227.5, 355.0, 80.0, 205.0, 347.5000000000009, 1000.0, 422.5, 249.9999999999991, 237.5, 387.5, 7.5, 527.5, 517.5, 597.5, 725.0, 500.0, 467.5, 422.5, 745.0, 485.0, 1165.0, 1025.0, 817.5, 470.0, 495.0,835.0, 497.5, 420.0, 490.0, 1225.0, 395.0, 260.0, 450.0, 535.0, 147.5, 17.5, 602.4999999999991, 660.0, 750.0, 312.5, 365.0, 505.0, 5032.5, 527.5, 372.5, 1125.0, 417.5,557.5, 425.0, 515.0, 165.0000000000009, 410.0, 502.5, 275.0, 527.5, 217.5, 527.5, 597.5, 325.0, 282.5000000000009, 520.0, 457.5, 522.5, 1410.0, 720.0, 405.0, 352.5, 410.0, 150.0, 452.5, 387.5]
    # listfpr = [0.33992094861660077, 0.1948529411764706, 0.3903345724907063, 0.42063492063492064, 0.27165354330708663, 0.39543726235741444, 0.11462450592885376, 0.17307692307692307, 0.050387596899224806, 0.1245136186770428, 0.1422924901185771, 0.0873015873015873, 0.23863636363636365, 0.14682539682539683, 0.0, 0.0, 0.4150197628458498, 0.3684210526315789, 0.0, 0.003968253968253968, 0.17063492063492064, 0.03968253968253968, 0.30514705882352944, 0.30943396226415093, 0.4037735849056604, 0.07936507936507936, 0.2261904761904762, 0.007575757575757576, 0.007905138339920948, 0.08300395256916997, 0.23015873015873015, 0.0, 0.233201581027668, 0.12359550561797752, 0.0, 0.03007518796992481, 0.08333333333333333, 0.07936507936507936, 0.09578544061302682, 0.09523809523809523, 0.05555555555555555, 0.29850746268656714, 0.15079365079365079, 0.5238095238095238, 0.0873015873015873, 0.25193798449612403, 0.2845849802371542, 0.0, 0.25660377358490566, 0.3241106719367589, 0.14285714285714285, 0.0873015873015873, 0.43873517786561267, 0.3492063492063492, 0.5079365079365079, 0.2619047619047619, 0.08661417322834646, 0.0, 0.16666666666666666, 0.09090909090909091, 0.4722222222222222, 0.17735849056603772, 0.1626984126984127, 0.3074074074074074, 0.15873015873015872, 0.49603174603174605, 0.2845849802371542, 0.06719367588932806, 0.1111111111111111, 0.0, 0.3611111111111111, 0.24603174603174602, 0.17857142857142858, 0.31851851851851853, 0.015873015873015872, 0.17857142857142858, 0.0, 0.36904761904761907, 0.26587301587301587, 0.31746031746031744, 0.3055555555555556, 0.18253968253968253, 0.007936507936507936, 0.3333333333333333, 0.391304347826087, 0.34375, 0.4523809523809524, 0.17857142857142858, 0.10714285714285714, 0.2896825396825397, 0.0, 0.06719367588932806, 0.07142857142857142, 0.00749063670411985, 0.003968253968253968, 0.4126984126984127, 0.0, 0.0311284046692607, 0.1422924901185771, 0.05952380952380952, 0.20238095238095238, 0.21825396825396826, 0.06746031746031746, 0.10317460317460317, 0.023809523809523808, 0.5198412698412699, 0.2857142857142857, 0.015873015873015872, 0.26022304832713755, 0.30039525691699603, 0.36904761904761907, 0.22924901185770752, 0.21031746031746032, 0.2706766917293233, 0.5402298850574713, 0.0873015873015873, 0.5793650793650794, 0.3134920634920635, 0.19172932330827067, 0.3611111111111111,0.1857707509881423, 0.3333333333333333, 0.36904761904761907, 0.15873015873015872, 0.1067193675889328, 0.0, 0.10714285714285714, 0.3373015873015873, 0.31716417910447764, 0.1568627450980392, 0.3492063492063492, 0.09230769230769231, 0.16412213740458015,0.4777777777777778, 0.0784313725490196, 0.24031007751937986, 0.375, 0.1417624521072797, 0.3531746031746032, 0.7075098814229249, 0.007380073800738007,0.0,0.26865671641791045, 0.09055118110236221, 0.04743083003952569, 0.003861003861003861, 0.06818181818181818,0.07142857142857142, 0.12790697674418605, 0.0873015873015873, 0.047619047619047616, 0.08914728682170543, 0.0, 0.07539682539682539, 0.1450980392156863, 0.1984126984126984, 0.01984126984126984, 0.03571428571428571, 0.1984126984126984, 0.19047619047619047, 0.08300395256916997, 0.2857142857142857, 0.4246031746031746, 0.23809523809523808,0.3531746031746032, 0.051587301587301584, 0.28846153846153844, 0.01509433962264151, 0.0, 0.2777777777777778, 0.5682656826568265, 0.13565891472868216, 0.003676470588235294, 0.39920948616600793, 0.0038910505836575876, 0.0648854961832061, 0.3025830258302583,0.3464566929133858, 0.20948616600790515, 0.38735177865612647, 0.21428571428571427, 0.07894736842105263, 0.27715355805243447, 0.4150197628458498, 0.49206349206349204,0.27734375, 0.1626984126984127, 0.03759398496240601, 0.003968253968253968, 0.1225296442687747, 0.3333333333333333, 0.2689393939393939, 0.03571428571428571, 0.019762845849802372, 0.26587301587301587,0.3382899628252788, 0.13157894736842105, 0.3204633204633205,0.28063241106719367, 0.2134387351778656, 0.2698412698412698, 0.11462450592885376, 0.03543307086614173, 0.27058823529411763, 0.2696629213483146, 0.36363636363636365,0.16666666666666666, 0.3373015873015873, 0.31746031746031744, 0.2777777777777778, 0.22727272727272727, 0.3333333333333333, 0.12403100775193798, 0.15019762845849802, 0.15476190476190477, 0.308300395256917, 0.1111111111111111, 0.25, 0.2896825396825397, 0.1746031746031746, 0.29924242424242425, 0.17391304347826086, 0.19607843137254902, 0.16853932584269662, 0.07936507936507936, 0.17803030303030304, 0.24110671936758893, 0.007380073800738007, 0.3134920634920635, 0.03571428571428571, 0.1984126984126984, 0.029304029304029304, 0.2964426877470356, 0.2845849802371542,0.0, 0.25650557620817843, 0.07114624505928854, 0.1349206349206349, 0.18146718146718147, 0.3843283582089552, 0.33969465648854963, 0.2740740740740741, 0.015873015873015872, 0.2608695652173913, 0.4268774703557312, 0.1111111111111111, 0.17063492063492064, 0.08015267175572519, 0.2835249042145594, 0.0, 0.07116104868913857, 0.01984126984126984, 0.13095238095238096, 0.2546816479400749, 0.1865079365079365, 0.6284584980237155,0.2037735849056604, 0.0, 0.12698412698412698, 0.023809523809523808, 0.06349206349206349, 0.3373015873015873, 0.14285714285714285, 0.15873015873015872, 0.32806324110671936, 0.023809523809523808, 0.007326007326007326, 0.27380952380952384, 0.11904761904761904, 0.0, 0.04365079365079365, 0.10622710622710622, 0.007936507936507936, 0.023715415019762844, 0.11320754716981132, 0.0, 0.3531746031746032, 0.031746031746031744, 0.16666666666666666, 0.3146067415730337]
    # listfnr = [0.08333333333333333, 0.1724137931034483, 0.3125, 0.02040816326530612, 0.0, 0.13157894736842105, 0.9791666666666666, 1.0, 0.9534883720930233, 0.4318181818181818, 0.9583333333333334, 0.9591836734693877, 0.4864864864864865, 0.9795918367346939, 0.9, 0.9166666666666666, 0.4375, 0.0, 0.8541666666666666, 0.5510204081632653, 0.9183673469387755, 0.8571428571428571, 0.10344827586206896, 0.7777777777777778, 0.0, 0.7142857142857143, 0.42857142857142855, 0.3783783783783784, 0.5833333333333334, 1.0, 0.0, 0.4375, 0.4583333333333333, 0.8235294117647058, 0.8, 0.2571428571428571, 0.4594594594594595, 0.42857142857142855, 1.0, 0.3877551020408163, 0.42857142857142855, 0.0, 0.40816326530612246, 0.0, 0.22448979591836735, 0.11627906976744186, 0.14583333333333334, 0.9795918367346939, 0.8333333333333334, 0.020833333333333332, 0.9183673469387755, 0.5714285714285714, 0.125, 0.0, 0.0, 0.3877551020408163, 0.9787234042553191, 0.9591836734693877, 0.32653061224489793, 0.6458333333333334, 0.04081632653061224, 0.08333333333333333, 1.0, 0.12903225806451613, 0.46938775510204084, 0.0, 0.3125, 0.7916666666666666, 0.5102040816326531, 0.6041666666666666, 0.1836734693877551, 1.0, 0.5102040816326531, 0.0, 0.9795918367346939, 0.5714285714285714, 0.9375, 0.3673469387755102, 0.30612244897959184, 0.0, 0.3673469387755102, 0.9795918367346939, 0.7551020408163265, 0.2653061224489796, 0.0, 0.3333333333333333, 0.0, 0.3877551020408163, 1.0, 0.3469387755102041, 0.9166666666666666, 0.9375, 0.42857142857142855, 1.0, 0.9183673469387755, 0.0, 0.9795918367346939, 1.0, 1.0, 0.9795918367346939, 0.673469387755102, 0.9591836734693877, 0.32653061224489793, 0.4897959183673469, 1.0, 0.3673469387755102, 0.9387755102040817, 0.9795918367346939, 0.09375, 0.16666666666666666, 0.6530612244897959, 0.3125, 0.9795918367346939, 0.08571428571428572, 0.025, 1.0, 0.04081632653061224, 0.2653061224489796, 0.6, 0.24489795918367346, 0.8958333333333334, 0.2653061224489796, 0.0, 0.7551020408163265, 0.7083333333333334, 0.8541666666666666, 0.7551020408163265, 0.12244897959183673, 0.0, 0.8043478260869565, 0.04081632653061224, 1.0, 0.358974358974359,0.0, 0.8478260869565217, 0.4186046511627907, 0.20689655172413793, 0.2, 0.04081632653061224, 0.0, 0.8666666666666667,0.8367346938775511,0.42424242424242425, 0.1276595744680851, 0.5833333333333334, 0.7619047619047619, 0.8378378378378378,0.32653061224489793, 0.3953488372093023, 0.4897959183673469, 0.673469387755102, 0.3953488372093023, 0.16326530612244897, 0.24489795918367346, 0.9347826086956522, 0.40816326530612246, 0.4897959183673469, 0.40816326530612246, 0.24489795918367346, 0.40816326530612246, 1.0, 0.5714285714285714, 0.42857142857142855, 0.3877551020408163,0.3877551020408163, 0.46938775510204084, 0.6585365853658537, 0.25, 0.9333333333333333, 0.04081632653061224, 0.0, 0.5581395348837209, 0.4827586206896552, 0.25, 0.9318181818181818, 0.8717948717948718, 0.0,0.2978723404255319, 0.4375, 0.0, 0.12244897959183673, 1.0, 0.47058823529411764, 0.3958333333333333, 0.5306122448979592,0.06666666666666667, 0.20408163265306123, 0.8, 0.7959183673469388, 0.7708333333333334, 0.40816326530612246, 0.24324324324324326, 0.8367346938775511, 0.5833333333333334, 1.0,0.0, 1.0, 0.5,0.3958333333333333, 0.5, 0.40816326530612246, 0.7708333333333334, 0.9148936170212766, 0.391304347826087, 0.23529411764705882, 0.4583333333333333,0.20408163265306123, 0.32653061224489793, 0.3877551020408163, 0.22448979591836735, 0.1891891891891892, 0.043478260869565216, 0.11627906976744186, 0.3958333333333333, 0.8163265306122449, 0.3333333333333333, 0.42857142857142855, 0.1836734693877551, 0.30612244897959184, 0.4897959183673469, 0.24324324324324326, 0.5416666666666666, 0.4782608695652174, 0.4117647058823529, 0.7551020408163265, 0.1891891891891892, 0.3541666666666667, 0.36666666666666664, 0.3877551020408163, 0.9591836734693877, 0.8367346938775511, 0.8571428571428571, 0.375, 0.3958333333333333,0.8979591836734694, 0.09375, 0.3958333333333333, 0.5510204081632653, 1.0, 0.0, 0.05128205128205128, 0.0, 0.9183673469387755, 0.1875, 0.08333333333333333, 0.4897959183673469, 0.6122448979591837, 0.8717948717948718, 0.175, 0.6122448979591837, 0.9705882352941176, 1.0, 0.6530612244897959, 0.058823529411764705, 0.9795918367346939, 0.4375,0.3055555555555556, 0.6326530612244898, 0.5714285714285714, 0.5714285714285714, 0.5306122448979592, 0.4897959183673469, 0.3673469387755102, 0.42857142857142855, 0.16666666666666666, 0.673469387755102, 0.6428571428571429, 0.4489795918367347, 0.3469387755102041, 0.9375, 0.8571428571428571, 0.7857142857142857, 1.0, 0.9375, 0.08333333333333333, 0.9142857142857143, 0.32653061224489793, 0.5306122448979592, 0.40540540540540543, 0.0]
    listlatency = [10.0, 20.0, 1165.0, 10.0, 5.0, 550.0, 0.0, 10.0, 5.0, 20.0, 230.0, 605.0, 2250.0, 4765.0, 15.0, 6950.0, 165.0, 795.0, 10.0, 765.0, 745.0, 80.00000000000045, 90.0, 705.0, 710.0, 1820.0, 5.0, 1070.0, 5695.0, 695.0, 145.0, 925.0, 660.0, 720.0, 670.0, 665.0, 70.0, 745.0, 10.0, 730.0, 10.0, 0.0, 20.0, 730.0, 15.0, 344.9999999999991, 270.0, 815.0, 10.0, 700.0, 530.0, 965.0, 0.0, 440.0, 105.0, 675.0, 15.0, 5.0, 10.0, 680.0, 20.0, 95.0, 690.0, 575.0, 10.0, 20.0, 705.0, 280.0, 570.0, 680.0, 455.0, 640.0, 360.0, 70.0, 625.0, 285.0, 585.0, 550.0, 5680.0, 680.0, 570.0, 300.0, 660.0, 690.0, 315.0, 545.0, 600.0, 90.0, 155.0, 370.0, 140.0, 640.0, 85.00000000000091, 725.0, 180.0, 190.0, 20.0, 545.0, 0.0, 10.0, 15.0, 5.0, 15.0, 20.0, 20.0, 175.0, 615.0, 985.0, 115.0, 695.0, 170.0, 20.0, 5.0, 40.0, 520.0, 115.0, 360.0, 15.0, 725.0, 10.0, 75.0, 410.0, 715.0, 20.0, 40.0, 0.0, 665.0, 730.0, 15.0, 1490.0, 460.0, 0.0, 15.0, 715.0, 10.0, 730.0, 145.0, 15.0, 525.0, 6135.0, 10.0, 650.0, 0.0, 525.0, 470.0, 650.0, 665.0, 5970.0, 380.0, 110.0, 595.0, 975.0, 7825.0, 0.0, 505.0, 20.0, 730.0, 555.0000000000009, 695.0, 0.0, 575.0, 10.0, 4845.0, 545.0, 20.0, 370.0, 710.0, 700.0, 615.0, 10.0, 115.0, 15.0, 500.0, 70.0, 1060.0, 15.00000000000091, 390.0, 250.0, 5.0, 20.0, 2850.000000000001, 185.0, 900.0, 135.0, 500.0, 305.0, 10.0, 360.0, 590.0, 15.0, 20.0, 5.0, 245.0, 700.0, 15.0, 355.0, 0.0, 295.0, 25.0, 60.0, 670.0, 0.0, 0.0, 730.0, 20.0, 15.0, 705.0, 10.0, 0.0, 2580.0, 1285.0, 190.0, 15.0, 345.0, 15.0, 270.0, 520.0, 5.0, 1715.0, 0.0, 710.0, 1280.0, 170.0, 635.0, 10.0, 65.0, 465.0, 15.0, 230.0, 635.0, 355.0, 1950.0, 105.0, 215.0, 15.0, 690.0, 5.0, 470.0, 1340.0, 220.0, 455.0, 730.0, 10.0, 135.0, 695.0, 0.0, 760.0, 10.0, 690.0, 40.0, 800.0, 665.0, 155.0, 185.0, 700.0, 155.0, 720.0, 10.0, 190.0, 730.0, 630.0, 1015.0, 460.0, 715.0, 15.0, 400.0, 730.0, 5.0, 0.0, 595.0, 145.0, 500.0, 555.0, 450.0, 15.0, 10.0, 15.0, 20.0, 0.0, 710.0, 10.0, 0.0, 795.0, 155.0, 1290.0, 125.0, 1430.0, 0.0, 40.0, 15.0, 655.0, 735.0000000000009, 10.0, 715.0, 715.0, 20.0, 275.0, 5980.0, 5.0, 10.0, 530.0, 15.0, 1505.0, 120.0, 1170.0, 920.0, 20.0, 20.0, 615.0, 750.0, 10.0, 15.0, 15.0, 1470.0, 15.0, 730.0000000000005, 655.0, 7000.0, 235.0, 405.0, 505.0, 1160.0, 305.0, 705.0, 515.0, 350.0, 355.0, 155.0, 7030.0, 1015.0, 5320.0, 305.0, 365.0, 490.0, 455.0, 675.0, 125.0, 585.0, 1365.0, 0.0, 740.0, 420.0, 715.0, 15.0, 45.0, 370.0, 60.0, 380.0, 5.0, 305.0, 165.0, 10.0, 400.0, 5815.0, 5165.0, 185.0, 30.0, 1010.0, 955.0, 540.0, 985.0, 20.0, 380.0, 760.0, 350.0, 705.0, 4490.0, 195.0, 15.0, 110.0, 2495.0, 35.0, 40.0, 1245.000000000001, 309.9999999999991, 5675.0, 915.0, 425.0, 5.0, 15.0, 785.0, 635.0, 790.0, 500.0, 660.0, 205.0, 150.0, 145.0, 0.0, 5.0, 515.0, 15.0, 0.0, 965.0, 20.0, 520.0, 225.0, 20.0, 540.0, 1040.0, 670.0, 470.0, 705.0, 575.0, 280.0, 5.0, 20.0, 1080.0, 365.0, 550.0, 405.0, 695.0, 1935.0, 110.0, 720.0, 20.0, 760.0, 1160.0, 150.0, 10.0, 270.0, 5.0, 1115.0, 45.0, 670.0, 705.0]
    listfpr = [0.14473684210526316, 0.23639774859287055, 0.06390977443609022, 0.19548872180451127, 0.2932330827067669, 0.31390977443609025, 0.3082706766917293, 0.2631578947368421, 0.22180451127819548, 0.3339587242026266, 0.37969924812030076, 0.12969924812030076, 0.08834586466165413, 0.09774436090225563, 0.09398496240601503, 0.009398496240601503, 0.07706766917293233, 0.016885553470919325, 0.2236842105263158, 0.07518796992481203, 0.11444652908067542, 0.20112781954887218, 0.2424812030075188, 0.10338345864661654, 0.07518796992481203, 0.08067542213883677, 0.14661654135338345, 0.0900562851782364, 0.001876172607879925, 0.1575984990619137, 0.2270168855534709, 0.11842105263157894, 0.18045112781954886, 0.0375234521575985, 0.013133208255159476, 0.02819548872180451, 0.2176360225140713, 0.0, 0.16541353383458646, 0.2706766917293233, 0.3101503759398496, 0.2236842105263158, 0.29831144465290804, 0.29887218045112784, 0.11654135338345864, 0.0, 0.12570356472795496, 0.20864661654135339, 0.35526315789473684, 0.0, 0.20300751879699247, 0.18233082706766918, 0.2387218045112782, 0.03759398496240601, 0.231203007518797, 0.3533834586466165, 0.21428571428571427, 0.19736842105263158, 0.2424812030075188, 0.4266917293233083, 0.2870544090056285, 0.0675422138836773, 0.16541353383458646, 0.041353383458646614, 0.2800751879699248, 0.19136960600375236, 0.10150375939849623, 0.14097744360902256, 0.28142589118198874, 0.11654135338345864, 0.19924812030075187, 0.16353383458646617, 0.26879699248120303, 0.09943714821763602, 0.05639097744360902, 0.39473684210526316, 0.15789473684210525, 0.22932330827067668, 0.2781954887218045, 0.15977443609022557, 0.1951219512195122, 0.29135338345864664, 0.02631578947368421, 0.06578947368421052, 0.23308270676691728, 0.16322701688555347, 0.23308270676691728, 0.25375939849624063, 0.36466165413533835, 0.23827392120075047, 0.2387218045112782, 0.35150375939849626, 0.37218045112781956, 0.11090225563909774, 0.17481203007518797, 0.2199248120300752, 0.48405253283302063, 0.22326454033771106, 0.03195488721804511, 0.40037593984962405, 0.3007518796992481, 0.5112781954887218, 0.2706766917293233, 0.225140712945591, 0.19324577861163228, 0.35526315789473684, 0.325187969924812, 0.11278195488721804, 0.37218045112781956, 0.28142589118198874, 0.48592870544090055, 0.30206378986866794, 0.20676691729323307, 0.29135338345864664, 0.1275797373358349, 0.27631578947368424, 0.2819548872180451, 0.18233082706766918, 0.2951127819548872, 0.26127819548872183, 0.3308270676691729, 0.231203007518797, 0.25, 0.2101313320825516, 0.38721804511278196, 0.41353383458646614, 0.20300751879699247, 0.046992481203007516, 0.2631578947368421, 0.34210526315789475, 0.17669172932330826, 0.20112781954887218, 0.3007518796992481, 0.25375939849624063, 0.09210526315789473, 0.212406015037594, 0.30393996247654786, 0.37406015037593987, 0.4191729323308271, 0.22556390977443608, 0.2575187969924812, 0.33270676691729323, 0.21052631578947367, 0.34962406015037595, 0.4277673545966229, 0.19172932330827067, 0.32142857142857145, 0.07879924953095685, 0.4323308270676692, 0.24812030075187969, 0.37523452157598497, 0.06766917293233082, 0.14097744360902256, 0.22932330827067668, 0.47368421052631576, 0.2045028142589118, 0.005639097744360902, 0.4981203007518797, 0.45778611632270166, 0.31203007518796994, 0.4323308270676692, 0.22180451127819548, 0.28893058161350843, 0.350844277673546, 0.35834896810506567, 0.25140712945590993, 0.12781954887218044, 0.2744360902255639, 0.19172932330827067, 0.2706766917293233, 0.2650375939849624, 0.231203007518797, 0.039473684210526314, 0.35834896810506567, 0.21428571428571427, 0.31390977443609025, 0.17481203007518797, 0.29135338345864664, 0.4774436090225564, 0.1801125703564728, 0.07706766917293233, 0.2161654135338346, 0.11278195488721804, 0.2706766917293233, 0.22556390977443608, 0.25375939849624063, 0.41353383458646614, 0.06015037593984962, 0.12218045112781954, 0.32894736842105265, 0.30206378986866794, 0.32706766917293234, 0.12570356472795496, 0.05451127819548872, 0.2781954887218045, 0.2199248120300752, 0.22180451127819548, 0.0300187617260788, 0.39097744360902253, 0.18045112781954886, 0.09380863039399624, 0.2199248120300752, 0.36466165413533835, 0.09586466165413533, 0.41275797373358347, 0.231203007518797, 0.015037593984962405, 0.11278195488721804, 0.15037593984962405, 0.05263157894736842, 0.16729323308270677, 0.0018796992481203006, 0.06203007518796992, 0.06566604127579738, 0.011278195488721804, 0.17636022514071295, 0.15947467166979362, 0.12030075187969924, 0.03571428571428571, 0.17669172932330826, 0.29887218045112784, 0.03383458646616541, 0.020637898686679174, 0.19924812030075187, 0.26127819548872183, 0.21428571428571427, 0.09022556390977443, 0.30639097744360905, 0.24060150375939848, 0.20300751879699247, 0.2387218045112782, 0.06015037593984962, 0.10714285714285714, 0.24812030075187969, 0.20488721804511278, 0.12593984962406016, 0.12406015037593984, 0.3827392120075047, 0.2857142857142857, 0.2551594746716698, 0.2349624060150376, 0.17105263157894737, 0.16353383458646617, 0.35150375939849626, 0.30956848030018763, 0.2894736842105263, 0.3082706766917293, 0.3383458646616541, 0.29887218045112784, 0.26127819548872183, 0.16917293233082706, 0.12781954887218044, 0.07518796992481203, 0.06578947368421052, 0.19548872180451127, 0.18045112781954886, 0.08630393996247655, 0.2932330827067669, 0.20488721804511278, 0.16541353383458646, 0.13721804511278196, 0.06203007518796992, 0.43045112781954886, 0.06203007518796992, 0.42293233082706766, 0.2387218045112782, 0.21052631578947367, 0.18609022556390978, 0.45112781954887216, 0.1726078799249531, 0.2870544090056285, 0.15413533834586465, 0.20300751879699247, 0.11278195488721804, 0.2669172932330827, 0.15037593984962405, 0.31203007518796994, 0.23076923076923078, 0.2236842105263158, 0.2650375939849624, 0.20112781954887218, 0.12218045112781954, 0.0600375234521576, 0.17669172932330826, 0.06578947368421052, 0.32142857142857145, 0.12030075187969924, 0.15601503759398497, 0.18421052631578946, 0.07142857142857142, 0.42105263157894735, 0.11090225563909774, 0.09022556390977443, 0.05639097744360902, 0.33646616541353386, 0.15196998123827393, 0.21052631578947367, 0.09398496240601503, 0.19172932330827067, 0.34774436090225563, 0.18984962406015038, 0.21804511278195488, 0.13157894736842105, 0.2776735459662289, 0.039399624765478425, 0.1894934333958724, 0.19887429643527205, 0.2776735459662289, 0.2706766917293233, 0.43045112781954886, 0.2236842105263158, 0.24436090225563908, 0.21804511278195488, 0.07879924953095685, 0.3176691729323308, 0.23684210526315788, 0.22180451127819548, 0.29887218045112784, 0.33270676691729323, 0.19924812030075187, 0.18984962406015038, 0.09586466165413533, 0.3101503759398496, 0.36654135338345867, 0.14473684210526316, 0.08646616541353383, 0.20676691729323307, 0.3233082706766917, 0.12969924812030076, 0.24812030075187969, 0.13133208255159476, 0.018796992481203006, 0.2199248120300752, 0.14285714285714285, 0.21428571428571427, 0.08458646616541353, 0.36278195488721804, 0.40977443609022557, 0.17105263157894737, 0.27631578947368424, 0.17669172932330826, 0.1951219512195122, 0.13157894736842105, 0.26127819548872183, 0.1350844277673546, 0.12570356472795496, 0.06578947368421052, 0.17105263157894737, 0.4718045112781955, 0.10902255639097744, 0.26127819548872183, 0.2199248120300752, 0.13721804511278196, 0.013157894736842105, 0.16541353383458646, 0.043233082706766915, 0.015037593984962405, 0.08082706766917293, 0.2894736842105263, 0.30639097744360905, 0.20300751879699247, 0.1801125703564728, 0.05639097744360902, 0.32706766917293234, 0.2349624060150376, 0.4116541353383459, 0.039473684210526314, 0.05628517823639775, 0.17293233082706766, 0.15601503759398497, 0.21575984990619138, 0.19548872180451127, 0.24812030075187969, 0.24953095684803, 0.18045112781954886, 0.16729323308270677, 0.09962406015037593, 0.21428571428571427, 0.231203007518797, 0.19548872180451127, 0.30451127819548873, 0.08082706766917293, 0.08082706766917293, 0.043233082706766915, 0.22556390977443608, 0.03759398496240601, 0.2349624060150376, 0.15384615384615385, 0.34962406015037595, 0.26127819548872183, 0.13345864661654136, 0.20300751879699247, 0.08082706766917293, 0.19736842105263158, 0.21388367729831145, 0.2026266416510319, 0.16165413533834586, 0.29831144465290804, 0.16353383458646617, 0.13721804511278196, 0.33771106941838647, 0.001876172607879925, 0.20676691729323307, 0.16917293233082706, 0.09022556390977443, 0.11090225563909774, 0.09568480300187618, 0.16917293233082706, 0.16917293233082706, 0.2800751879699248, 0.2951127819548872, 0.28893058161350843, 0.09398496240601503, 0.043233082706766915, 0.3227016885553471, 0.1894934333958724, 0.020676691729323307, 0.2349624060150376, 0.18796992481203006, 0.12030075187969924, 0.2045028142589118, 0.14661654135338345, 0.09962406015037593, 0.1651031894934334, 0.17636022514071295, 0.2462406015037594] 
    listfnr = [0.12244897959183673, 0.10416666666666667, 0.9795918367346939, 0.16326530612244897, 0.3469387755102041, 0.46938775510204084, 0.0, 0.10204081632653061, 0.0, 0.041666666666666664, 0.1836734693877551, 0.8571428571428571, 1.0, 1.0, 0.673469387755102, 1.0, 0.8979591836734694, 0.9791666666666666, 0.0, 0.8367346938775511, 0.6041666666666666, 0.061224489795918366, 0.16326530612244897, 0.6122448979591837, 0.8571428571428571, 1.0, 0.02040816326530612, 0.9583333333333334, 1.0, 0.5625, 0.3958333333333333, 0.9795918367346939, 0.9387755102040817, 0.9791666666666666, 0.8958333333333334, 0.9795918367346939, 0.08333333333333333, 0.8541666666666666, 0.42857142857142855, 0.5918367346938775, 0.14285714285714285, 0.0, 0.0, 0.5918367346938775, 0.9183673469387755, 0.7708333333333334, 0.4166666666666667, 0.6530612244897959, 0.0, 0.9795918367346939, 0.42857142857142855, 0.7755102040816326, 0.10204081632653061, 0.673469387755102, 0.12244897959183673, 0.5918367346938775, 0.14285714285714285, 0.02040816326530612, 0.42857142857142855, 0.5510204081632653, 0.0, 0.5625, 0.5510204081632653, 0.4897959183673469, 0.1836734693877551, 0.3333333333333333, 0.6122448979591837, 0.9795918367346939, 0.4583333333333333, 0.5510204081632653, 0.46938775510204084, 0.5102040816326531, 0.673469387755102, 0.5625, 0.5306122448979592, 0.46938775510204084, 0.6530612244897959, 0.46938775510204084, 1.0, 0.5510204081632653, 0.4791666666666667, 0.32653061224489793, 0.9387755102040817, 0.5510204081632653, 0.24489795918367346, 0.9583333333333334, 0.4897959183673469, 0.5306122448979592, 0.12244897959183673, 0.375, 0.16326530612244897, 0.5306122448979592, 0.16326530612244897, 0.9795918367346939, 0.8367346938775511, 0.16326530612244897, 0.0, 0.9166666666666666, 0.2653061224489796, 0.16326530612244897, 0.061224489795918366, 0.04081632653061224, 0.10204081632653061, 0.375, 0.0, 0.14285714285714285, 0.4897959183673469, 0.8367346938775511, 0.08163265306122448, 0.9583333333333334, 0.3541666666666667, 0.0625, 0.42857142857142855, 0.02040816326530612, 0.4375, 0.08163265306122448, 0.9591836734693877, 0.0, 0.5918367346938775, 0.04081632653061224, 0.061224489795918366, 0.3673469387755102, 0.7959183673469388, 0.020833333333333332, 0.02040816326530612, 0.0, 0.5918367346938775, 0.5918367346938775, 0.14285714285714285, 1.0, 0.5306122448979592, 0.32653061224489793, 0.04081632653061224, 0.9795918367346939, 0.20408163265306123, 0.8367346938775511, 0.20833333333333334, 0.0, 0.42857142857142855, 1.0, 0.24489795918367346, 0.5306122448979592, 0.0, 0.42857142857142855, 0.4791666666666667, 0.9591836734693877, 0.5306122448979592, 1.0, 0.30612244897959184, 0.40816326530612246, 0.4791666666666667, 0.7959183673469388, 1.0, 0.14285714285714285, 0.42857142857142855, 0.25, 0.9387755102040817, 0.4489795918367347, 0.5625, 0.0, 0.46938775510204084, 0.0, 1.0, 0.4583333333333333, 0.0, 0.375, 0.5714285714285714, 0.8367346938775511, 0.8979591836734694, 0.6938775510204082, 0.12244897959183673, 0.0, 0.8571428571428571, 0.6041666666666666, 0.8571428571428571, 0.0, 0.3673469387755102, 0.24489795918367346, 0.20408163265306123, 0.16666666666666666, 1.0, 0.6530612244897959, 0.9387755102040817, 0.2653061224489796, 0.42857142857142855, 0.24489795918367346, 0.0, 0.8571428571428571, 0.8367346938775511, 0.0, 0.125, 0.0, 0.4583333333333333, 0.9795918367346939, 0.20408163265306123, 0.2857142857142857, 0.6122448979591837, 0.9791666666666666, 0.14285714285714285, 0.061224489795918366, 0.9583333333333334, 0.2857142857142857, 0.02040816326530612, 0.9183673469387755, 0.25, 0.0, 0.8163265306122449, 0.4489795918367347, 0.1836734693877551, 1.0, 1.0, 0.32653061224489793, 0.42857142857142855, 0.9791666666666666, 0.1836734693877551, 0.22916666666666666, 0.4583333333333333, 0.0, 1.0, 0.02040816326530612, 0.6122448979591837, 1.0, 0.16666666666666666, 0.9387755102040817, 0.10204081632653061, 0.2857142857142857, 0.4897959183673469, 0.0, 0.1836734693877551, 0.5102040816326531, 0.40816326530612246, 1.0, 0.3469387755102041, 0.16326530612244897, 0.4897959183673469, 0.673469387755102, 0.04081632653061224, 0.5208333333333334, 1.0, 0.16666666666666666, 0.5714285714285714, 0.8979591836734694, 0.3877551020408163, 0.3877551020408163, 0.625, 0.32653061224489793, 0.6122448979591837, 0.14285714285714285, 0.6326530612244898, 0.3877551020408163, 0.9795918367346939, 0.5306122448979592, 0.5510204081632653, 0.6530612244897959, 0.673469387755102, 0.1836734693877551, 0.875, 0.32653061224489793, 0.3469387755102041, 0.6326530612244898, 0.5102040816326531, 0.9795918367346939, 0.3673469387755102, 0.9795918367346939, 0.0, 0.4489795918367347, 0.7551020408163265, 0.08163265306122448, 0.32653061224489793, 0.7291666666666666, 0.10416666666666667, 0.673469387755102, 0.5714285714285714, 0.3877551020408163, 0.0, 0.42857142857142855, 0.0, 0.14583333333333334, 0.0, 0.9795918367346939, 0.32653061224489793, 0.5510204081632653, 0.9583333333333334, 0.2653061224489796, 1.0, 0.2857142857142857, 1.0, 0.10204081632653061, 0.1836734693877551, 0.30612244897959184, 0.5306122448979592, 0.5918367346938775, 0.40816326530612246, 0.5714285714285714, 0.5714285714285714, 0.25, 0.24489795918367346, 1.0, 0.0, 0.0, 0.5918367346938775, 0.30612244897959184, 1.0, 0.125, 0.9583333333333334, 0.75, 0.020833333333333332, 0.020833333333333332, 0.5102040816326531, 0.6122448979591837, 0.14285714285714285, 0.061224489795918366, 0.0, 1.0, 0.30612244897959184, 0.9387755102040817, 0.5510204081632653, 1.0, 0.22448979591836735, 0.32653061224489793, 0.40816326530612246, 0.9795918367346939, 0.40816326530612246, 0.5714285714285714, 0.9387755102040817, 0.6326530612244898, 0.673469387755102, 0.16326530612244897, 1.0, 0.9795918367346939, 1.0, 0.3673469387755102, 0.40816326530612246, 0.3877551020408163, 0.3673469387755102, 0.5510204081632653, 0.16326530612244897, 0.46938775510204084, 1.0, 0.8775510204081632, 0.8163265306122449, 0.375, 0.7346938775510204, 0.1836734693877551, 0.020833333333333332, 0.2916666666666667, 0.46938775510204084, 0.32653061224489793, 0.0, 0.4489795918367347, 0.22448979591836735, 0.0, 0.5510204081632653, 1.0, 1.0, 0.5714285714285714, 0.8367346938775511, 0.8163265306122449, 0.9387755102040817, 0.673469387755102, 0.8367346938775511, 0.3333333333333333, 0.6530612244897959, 0.6122448979591837, 0.5714285714285714, 0.5714285714285714, 1.0, 0.9791666666666666, 0.10204081632653061, 0.22448979591836735, 1.0, 0.02040816326530612, 0.20408163265306123, 1.0, 0.30612244897959184, 1.0, 0.7346938775510204, 0.3469387755102041, 0.12244897959183673, 0.08163265306122448, 0.6326530612244898, 0.8367346938775511, 0.9183673469387755, 0.9795918367346939, 0.5306122448979592, 0.6530612244897959, 0.16326530612244897, 0.1875, 0.04081632653061224, 0.02040816326530612, 0.42857142857142855, 0.32653061224489793, 0.3469387755102041, 0.7755102040816326, 0.6875, 0.5625, 0.20408163265306123, 0.16666666666666666, 0.42857142857142855, 0.8367346938775511, 0.5416666666666666, 0.875, 0.7551020408163265, 0.46938775510204084, 0.30612244897959184, 0.5102040816326531, 0.8125, 0.8775510204081632, 0.32653061224489793, 0.4489795918367347, 0.4489795918367347, 0.5625, 1.0, 0.2653061224489796, 0.5833333333333334, 0.25, 0.6122448979591837, 0.9591836734693877, 0.6122448979591837, 0.4489795918367347, 0.4166666666666667, 0.0, 0.8979591836734694, 0.3958333333333333, 0.5416666666666666, 0.5714285714285714]  
    co = 0 
    no_latency = 0 
    number_of_samples = 0  
    
    for test_subj in SA_id[6:15] : # leave-one-subject-out
        file_name = "/Users/brisamanee/Desktop/BOSSVS FALL/DRETUNE2-EXP 3 RESULTS/{}ResultsWEASEL_MUSE EXP3.txt".format(test_subj)
        f = open( file_name ,'w')
        print('\n===================================')
        print('test subject:', test_subj)
        f.write('TEST SUBJECT :{}\n'.format(str(test_subj))) 
        learn_idxs = np.where(meta[:,2] != test_subj)[0] # list of learning index
        test_idxs = np.where(meta[:,2] == test_subj)[0] # list of test index
        X_learn, y_learn, meta_learn = X[learn_idxs], y[learn_idxs], meta[learn_idxs]
        
        print( X_learn.shape )
        print( y_learn.shape )

        filename = '/Users/brisamanee/Desktop/BOSSVS FALL/Model/RETUNE-WEASELMUSEMODEL {}.sav'.format(test_subj)
        # if not os.path.isdir(filename) : 
        #     print("hello")
        #     muse = WEASELMUSE(word_size = 4, window_sizes=window_lengths , strategy='entropy')
        #     logistic  = LogisticRegression() 
        #     pipe = Pipeline(steps=[("muse", muse ), ("logistic", logistic)])

        #     pipe.fit(X_learn , y_learn)
        #     pickle.dump(pipe, open(filename, 'wb'))


        # muse = WEASELMUSE(word_size = 4  , window_sizes=window_lengths , strategy='entropy')
        # logistic  = LogisticRegression() 
        # pipe = Pipeline(steps=[("muse", muse ), ("logistic", logistic)])
        # pipe.fit(X_learn, y_learn)
        # filename = 'WEASELMUSEModel SA{}.sav'.format(test_subj)
        # pickle.dump(pipe, open(filename, 'wb'))

        X_test, y_test, meta_test = X[test_idxs], y[test_idxs], meta[test_idxs]
        fullX_test = fullX[test_idxs]

        # # #loop through each activity, trial 
        n , m , t = X_test.shape
        model = pickle.load(open(filename, 'rb'))

        f.close() 

        sublatency = [ ] 
        subfpr =  [ ]
        subfnr = [ ] 

        for sample in range(current_sample , n) : 
            number_of_samples +=1 
    
            f = open( file_name ,'a')
            print(X_test[sample, 0 ,:].shape)
            print(meta_test[sample][0])

            if meta_test[sample][0] == 'D' : 
                continue 

            print(meta_test[sample])
            print(type(meta_test[sample ]) )
            sample_name = meta_test[sample][0] +  meta_test[sample][1] + meta_test[sample][2] + meta_test[sample][3]
            print(meta_test[sample][0] + meta_test[sample][1])
            f.write(sample_name + '\n') 
            print( type( meta_test )) 

            # #process each sample 
            the_array = np.array(X_test[sample, 0 , :]) 
            max_xaccidx_col = np.argmax(np.abs(the_array), axis=0)
            print(X_test[sample ,  1 ,  : ])
            the_array = np.array(X_test[sample ,1 , : ])
            max_zaccidx_col = np.argmax(np.abs(the_array), axis=0)  

            new_ts = np.array(X_test[sample, : , :])
            am , ap = new_ts.shape 

            max_peak_index = 0 
            max_peak_value = -1 

            for ind in range( ap ) :
            
                
                if (fullX_test[sample , 0 , ind ]**2 + fullX_test[sample,1, ind]**2 + fullX_test[sample , 2  , ind ]**2) > max_peak_value : 
                    max_peak_value = fullX_test[sample , 0 , ind ]**2 + fullX_test[sample,1, ind]**2 + fullX_test[sample , 2  , ind ]**2
                    max_peak_index = ind

            peak_time =  ( max_xaccidx_col + max_zaccidx_col ) / 2
            # print(max_peak_index , peak_time)
            peak_time = max_peak_index
            
            start = peak_time - 143 #index basis 

            end = peak_time + 100
            print(start , end ) 

            if start < 100 : 
                co += 1 

            #measure latency , fpr , fnr 

            st = start / 200 * 1000 #literal time point basis 
            en = end / 200 * 1000 
            
            # inc_val = 100 
            length_time = []
            prob_score = []

            positive = 0 
            negative = 0
            falsepos = 0 
            falseneg = 0 
            latency =  [ ]
            
            window_length = 1000

            print("X_TEST SHAPE ")
            print(X_test.shape)
            #post padding
            #sliding window : 5 (pre-pad if t before 5 of (5 - t) zeroes) , overlap 25ms 
            for space in range( 100 , window_length , 5 ) : 

                # sample_test = np.array( X_test[ sample , : ,  max(0,space - window_length   + 1  ) : space ]).reshape(1, 5, window_length - 1 )
                sample_test = np.array( X_test[ sample , : ,  0 : space ]).reshape(1, 5, space )
                print(sample_test.shape )

                shape = np.shape( sample_test )
                print(shape)
                padded_array  = np.zeros((1 , 5 , 36000 ))

                padded_array[:shape[0] , :shape[1] , shape[2] - 1 :shape[2] + shape[2] - 1 ] = sample_test
                sample_test = padded_array

                print(sample_test)

                #y_pred = model.predict(sample_test) 
    
                probs = model.predict_proba(sample_test) 

                t = (space * 1 / 200)*1000
                predict_result = probs[0][1]

                if predict_result >= threshold : 
                    y_pred = 'F'
                else :
                    y_pred = 'D'

                
                length_time.append((space * 1 / 200)*1000 )
                prob_score.append(predict_result)

                if probs[0][1] >= threshold : 
                    latency.append(length_time[-1] - st )

                if t  >= st and t <= en :
                    positive += 1 
                    if y_pred == 'D' :
                        falseneg += 1 
                        
                else : 
                    negative += 1 
                    if y_pred == 'F' : 
                        falsepos += 1

            for space in range( window_length , 3001 , 5 ) :
                sample_test = np.array( X_test[ sample , : ,  max(0,space - window_length   + 1  ) : space ]).reshape(1, 5, window_length - 1 )

                print(sample_test.shape )

                shape = np.shape( sample_test )
                print(shape)
                padded_array  = np.zeros((1 , 5 , 36000))
                
                padded_array[:shape[0] , :shape[1] , :shape[2]] = sample_test
                sample_test = padded_array 

                print(sample_test)

                y_pred = model.predict(sample_test)
    
                probs = model.predict_proba(sample_test) 

                t = (space * 1 / 200)*1000
                predict_result = probs[0][1] 
                
                if probs[0][1] >= threshold : 
                    y_pred = 'F'
                else :
                    y_pred = 'D'

                length_time.append((space * 1 / 200)*1000 )
                prob_score.append(probs[0][1])

                if probs[0][1] >= threshold : 
                    latency.append(length_time[-1] - st )

                if t  >= st and t <= en :
                    positive += 1 
                    if y_pred == 'D' :
                        falseneg += 1 

                else : 
                    negative += 1 

                    if y_pred == 'F' : 
                        falsepos += 1

            plt.plot(length_time, prob_score , color = 'blue')
        
            plt.plot(length_time, prob_score , color = 'blue')
            plt.ylim([0,1])

            plt.xlim([0 , 16000])
            plt.hlines(threshold , 0 , 16000 , linestyle  = 'dashed')
            plt.vlines(st, 0 , 1  , color = 'goldenrod', linestyle = 'dashed',label='Falling Phase Start/End')
            plt.vlines(en,0 , 1 , color = 'goldenrod', linestyle = 'dashed')
            plt.xlabel("Relative Time (ms)")
            plt.ylabel("Output Probability of Whether Input Is a Fall or Not ")
            plt.legend() 

            plt.savefig("/Users/brisamanee/Desktop/BOSSVS FALL/DRETUNE2-PLOT/WEASEL+MUSE{}.png".format(sample_name))
            plt.clf() 
            # plt.show()

            if not ( positive == 0 or negative == 0 )  : 
         
                fpr = falsepos/ ( negative ) 
                fnr = falseneg / ( positive  ) 

            else : 
                continue 
        
            print( fpr , fnr  , latency )
            have = False 
            for i in latency : 
                
                if i >= 0 : 
                    have = True 
                    new_latency = i 
                    new_fpr =  fpr 
                    new_fnr =  fnr

                    f.write("latency : {}\n ".format(new_latency))
                    listlatency.append(new_latency )
                    listfpr.append(new_fpr)
                    listfnr.append(new_fnr)
                    sublatency.append(new_latency)
                    subfpr.append(new_fpr)
                    subfnr.append(new_fnr)
                    f.write("OVERALL LIST :\n")
                    f.write("LATENCY" + str( listlatency ) + "\n")
                    f.write("FPR {} \n".format(listfpr))
                    f.write("FNR {} \n".format(listfnr))
                    f.write("SUB LIST : \n")
                    f.write("LATENCY" + str( sublatency ) + "\n")
                    f.write("FPR {} \n".format(subfpr))
                    f.write("FNR {} \n".format(subfnr))
             

                    break

                else : 
                    f.write("NO LATENCY")

            
            if not have : 
                no_latency += 1 

            f.write("no_latency:{}\n".format(no_latency))
            f.write("discard early:{}\n".format(co))
            f.write("number of samples:{}\n".format(number_of_samples))
            f.write("SAMPLE index : {} \n".format(str(sample)))
            
            #save current stage sum results 
            f.close() 
        

        f = open( file_name , 'a')
        avgsublatency  = sum(sublatency)  / len(sublatency )
        avgsubfpr = sum(subfpr ) / len(subfpr)
        avgsubfnr = sum(subfnr) / len(subfnr )


        sdsublatency =  sqrt( sum (( i - avgsublatency )**2  for i in sublatency)/ len(sublatency )) 
        sdsubfpr =  sqrt( sum(( i - avgsubfpr )**2 for i in subfpr) / len(subfpr )) 
        sdsubfnr = sqrt( sum(( i - avgsubfnr )**2  for i in subfnr) / len(subfnr )) 

        f.write("AVERAGE SUBJECT RESULTS : \n")
        f.write("Latency  : {} sd : {} ".format(avgsublatency , sdsublatency  ))
        f.write("FPR   : {} sd : {} ".format(avgsubfpr , sdsubfpr  ))
        f.write("FNR   : {} sd : {} ".format(avgsubfnr , sdsubfnr  ))

        f.close()

    f = open("/Users/brisamanee/Desktop/BOSSVS FALL/DRETUNE2-EXP 3 RESULTS/EXP3WEASEL_MUSEOVERALL-RESULTS.txt" , 'w')

    avglatency  = sum(listlatency)  / len(listlatency )
    avgfpr = sum(listfpr ) / len(listfpr)
    avgfnr = sum(subfnr) / len(listfnr )

    sdlatency =  sqrt( sum(( i - avglatency )**2 for i in listlatency)/ len(listlatency ) ) 
    sdfpr =  sqrt( sum(( i - avgfpr )**2 for i in listfpr)/ len(listfpr )) 
    sdfnr = sqrt( sum(( i - avgfnr )**2  for i in listfnr)/ len(listfnr  ))

    f.write("AVERAGE OVERALL RESULTS : \n")
    f.write("Latency  : {} sd : {} ".format(avglatency , sdlatency  ))
    f.write("FPR   : {} sd : {} ".format(avgfpr , sdfpr  ))
    f.write("FNR   : {} sd : {} ".format(avgfnr , sdfnr  ))
    f.write("Count of discarded sample from early : {} ".format(co))
    f.write("Count number of no latency : {}".format(no_latency))
    f.write("no_latency:{}\n".format(no_latency))
    f.write("discard early:{}\n".format(co))
    f.write("number of samples:{}\n".format(number_of_samples))


    f.close()

